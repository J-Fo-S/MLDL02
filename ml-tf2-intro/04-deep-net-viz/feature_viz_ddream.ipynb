{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"feature_viz_ddream.ipynb","provenance":[]},"kernelspec":{"display_name":"Python [Root]","language":"python","name":"Python [Root]"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xu2SVpFJjmJr"},"source":["# DeepDreaming with TensorFlow"]},{"cell_type":"markdown","metadata":{"id":"hupz2hrZjdnC","colab_type":"toc"},"source":[">[Loading the model graph](#loading)\n","\n",">[Naive feature visualization](#naive)\n","\n",">[Multiscale image generation](#multiscale)\n","\n",">[Laplacian Pyramid Gradient Normalization](#laplacian)\n","\n",">[Playing with feature visualzations](#playing)\n","\n",">[DeepDream](#deepdream)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-PLC9SvcQgkG"},"source":["This notebook demonstrates a number of Convolutional Neural Network image generation techniques implemented with TensorFlow for fun and science:\n","\n","- visualize individual feature channels and their combinations to explore the space of patterns learned by the neural network (see [GoogLeNet](http://storage.googleapis.com/deepdream/visualz/tensorflow_inception/index.html) and [VGG16](http://storage.googleapis.com/deepdream/visualz/vgg16/index.html) galleries)\n","- embed TensorBoard graph visualizations into Jupyter notebooks\n","- produce high-resolution images with tiled computation ([example](http://storage.googleapis.com/deepdream/pilatus_flowers.jpg))\n","- use Laplacian Pyramid Gradient Normalization to produce smooth and colorful visuals at low cost\n","- generate DeepDream-like images with TensorFlow (DogSlugs included)\n","\n","\n","The network under examination is the [GoogLeNet architecture](https://arxiv.org/pdf/1409.4842.pdf), trained to classify images into one of 1000 categories of the [ImageNet](http://image-net.org/) dataset. It consists of a set of layers that apply a sequence of transformations to the input image. The parameters of these transformations were determined during the training process by a variant of gradient descent algorithm. The internal image representations may seem obscure, but it is possible to visualize and interpret them. In this notebook we are going to present a few tricks that allow to make these visualizations both efficient to generate and even beautiful. Impatient readers can start with exploring the full galleries of images generated by the method described here for [GoogLeNet](http://storage.googleapis.com/deepdream/visualz/tensorflow_inception/index.html) and [VGG16](http://storage.googleapis.com/deepdream/visualz/vgg16/index.html) architectures."]},{"cell_type":"code","metadata":{"id":"jtD9nb-2QgkY","cellView":"both","executionInfo":{"status":"ok","timestamp":1604240376082,"user_tz":-480,"elapsed":6037,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}},"outputId":"c5d3951d-c828-4ab0-db8d-918c7e11f4eb","colab":{"base_uri":"https://localhost:8080/"}},"source":["%tensorflow_version 1.x\n","# boilerplate code\n","from __future__ import print_function\n","import os\n","from io import BytesIO\n","import numpy as np\n","from functools import partial\n","import PIL.Image\n","from IPython.display import clear_output, Image, display, HTML\n","import matplotlib.pyplot as plt\n","from skimage.transform import resize\n","\n","import tensorflow as tf"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ILvNKvMvc2n5"},"source":["<a id='loading'></a>\n","## Loading the model graph\n","\n","The pretrained network can be downloaded [here](https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip). Unpack the `tensorflow_inception_graph.pb` file from the archive and set its path to `model_fn` variable. Alternatively you can uncomment and run the following cell to download the network:"]},{"cell_type":"code","metadata":{"id":"-cAnBw9tpT8O","executionInfo":{"status":"ok","timestamp":1604240376083,"user_tz":-480,"elapsed":6034,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["#!wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip && unzip inception5h.zip"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kJuJRLiQgkg","cellView":"both","executionInfo":{"status":"error","timestamp":1604240573094,"user_tz":-480,"elapsed":790,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}},"outputId":"cdd3b058-edbb-4a98-9e7b-b3b37fada4ef","colab":{"base_uri":"https://localhost:8080/","height":429}},"source":["#sess.close()\n","model_fn = 'tensorflow_inception_graph.pb'\n","\n","# creating TensorFlow session and loading the model from the model_fn file \n","graph = tf.Graph()\n","sess = tf.InteractiveSession(graph=graph)\n","with tf.gfile.FastGFile(model_fn, 'rb') as f:\n","    graph_def = tf.GraphDef()\n","    graph_def.ParseFromString(f.read())\n","t_input = tf.placeholder(np.float32, name='input') # define the input tensor\n","imagenet_mean = 117.0\n","t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\n","tf.import_graph_def(graph_def, {'input':t_preprocessed})"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"],"name":"stderr"},{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-bd46007653e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# define the input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimagenet_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m117.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                            \"File isn't open for reading\")\n\u001b[1;32m     83\u001b[0m       self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 84\u001b[0;31m           compat.as_bytes(self.__name), 1024 * 512)\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: tensorflow_inception_graph.pb; No such file or directory"]}]},{"cell_type":"code","metadata":{"id":"dyICafW1pT8V","executionInfo":{"status":"aborted","timestamp":1604240378548,"user_tz":-480,"elapsed":8485,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["\n","# Helper functions for TF Graph visualization\n","\n","def strip_consts(graph_def, max_const_size=32):\n","    \"\"\"Strip large constant values from graph_def.\"\"\"\n","    strip_def = tf.GraphDef()\n","    for n0 in graph_def.node:\n","        n = strip_def.node.add() \n","        n.MergeFrom(n0)\n","        if n.op == 'Const':\n","            tensor = n.attr['value'].tensor\n","            size = len(tensor.tensor_content)\n","            if size > max_const_size:\n","                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n","    return strip_def\n","  \n","def rename_nodes(graph_def, rename_func):\n","    res_def = tf.GraphDef()\n","    for n0 in graph_def.node:\n","        n = res_def.node.add() \n","        n.MergeFrom(n0)\n","        n.name = rename_func(n.name)\n","        for i, s in enumerate(n.input):\n","            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n","    return res_def\n","  \n","def show_graph(graph_def, max_const_size=32):\n","    \"\"\"Visualize TensorFlow graph.\"\"\"\n","    if hasattr(graph_def, 'as_graph_def'):\n","        graph_def = graph_def.as_graph_def()\n","    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n","    code = \"\"\"\n","        <script>\n","          function load() {{\n","            document.getElementById(\"{id}\").pbtxt = {data};\n","          }}\n","        </script>\n","        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n","        <div style=\"height:600px\">\n","          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n","        </div>\n","    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n","  \n","    iframe = \"\"\"\n","        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n","    \"\"\".format(code.replace('\"', '&quot;'))\n","    display(HTML(iframe))\n","\n","# Visualizing the network graph. Be sure expand the \"mixed\" nodes to see their \n","# internal structure. We are going to visualize \"Conv2D\" nodes.\n","tmp_def = rename_nodes(graph_def, lambda s:\"/\".join(s.split('_',1)))\n","show_graph(tmp_def)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJZVMSmiQgkp"},"source":["To take a glimpse into the kinds of patterns that the network learned to recognize, we will try to generate images that maximize the sum of activations of particular channel of a particular convolutional layer of the neural network. The network we explore contains many convolutional layers, each of which outputs tens to hundreds of feature channels, so we have plenty of patterns to explore."]},{"cell_type":"code","metadata":{"id":"LrucdvgyQgks","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378549,"user_tz":-480,"elapsed":8475,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["layers = [op.name for op in graph.get_operations() if op.type=='Conv2D' and 'import/' in op.name]\n","feature_nums = [int(graph.get_tensor_by_name(name+':0').get_shape()[-1]) for name in layers]\n","\n","print('Number of layers', len(layers))\n","print('Total number of feature channels:', sum(feature_nums))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdmS0KNFpT8e","executionInfo":{"status":"aborted","timestamp":1604240378550,"user_tz":-480,"elapsed":8474,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["def T(layer):\n","    '''Helper for getting layer output tensor'''\n","    return graph.get_tensor_by_name(\"import/%s:0\"%layer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgnmJo9PpT8g","executionInfo":{"status":"aborted","timestamp":1604240378552,"user_tz":-480,"elapsed":8466,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["# TO-DO : fix index calling so import is not twice printed with T(layer)\n","\n","layer=layers[0]\n","print(layer)\n","layer = layer.split(\"/\")[1]\n","print(layer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XdaV9v9jpT8k","executionInfo":{"status":"aborted","timestamp":1604240378553,"user_tz":-480,"elapsed":8457,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["T(layer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X36-ZlOKpT8n","executionInfo":{"status":"aborted","timestamp":1604240378554,"user_tz":-480,"elapsed":8448,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["for l, layer in enumerate(layers):\n","    layer = layer.split(\"/\")[1]\n","    num_channels = T(layer).shape[3]\n","    print(l, layer, num_channels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nv2JqNLBhy1j"},"source":["<a id='naive'></a>\n","## Naive feature visualization"]},{"cell_type":"markdown","metadata":{"id":"6LXaGEJkQgk4"},"source":["Let's start with a naive way of visualizing these. Image-space gradient ascent!"]},{"cell_type":"code","metadata":{"id":"65IbP3JApT8s","executionInfo":{"status":"aborted","timestamp":1604240378554,"user_tz":-480,"elapsed":8438,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["# Picking some internal layer. Note that we use outputs before applying the ReLU nonlinearity\n","# to have non-zero gradients for features with negative initial activations.\n","layer = 'mixed4d_3x3_bottleneck_pre_relu'\n","print(layers[37])\n","channel = 139 # picking some feature channel to visualize\n","\n","# start with a gray image with a little noise\n","img_noise = np.random.uniform(size=(224,224,3)) + 100.0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZxC_XGGXQgk7","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378555,"user_tz":-480,"elapsed":8429,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["\n","def showarray(a, fmt='jpeg'):\n","    '''create a jpeg file from an array a and visualize it'''\n","    # clip the values to be between 0 and 255\n","    a = np.uint8(np.clip(a, 0, 1)*255)\n","    f = BytesIO()\n","    PIL.Image.fromarray(a).save(f, fmt)\n","    display(Image(data=f.getvalue()))\n","    \n","def visstd(a, s=0.1):\n","    '''Normalize the image range for visualization'''\n","    return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5\n","\n","\n","def render_naive(t_obj, img0=img_noise, iter_n=20, step=1.0):\n","    \n","    # t_obj: is the featuremap (or maps) where we want to mixamise the activities of the neurons e.g. T(layer)[:,:,:,channel]\n","    # img0: is the input image e.g. random noise or a cat\n","    # iter_n: number of iterations in the gradient ascent\n","    # step: step size of the gradient\n","    \n","    t_score = tf.reduce_mean(t_obj) # defining the optimization objective (mean of the neuron activities in t_obj)\n","    t_grad = tf.gradients(t_score, t_input)[0] # calculate the gradient of the objective function!!!\n","    \n","    img = img0.copy()\n","    showarray(visstd(img)) # show the input image\n","    \n","    for i in range(iter_n): # for iter_n iterations keep updating the image\n","        g, score = sess.run([t_grad, t_score], {t_input:img})\n","        # normalizing the gradient, so the same step size should work \n","        g /= g.std()+1e-8         # for different layers and networks\n","        img += g*step\n","        print(i, score, end = ' ') # show the current objective value\n","        showarray(visstd(img)) # show the actual image\n","    #clear_output()\n","    \n","\n","render_naive(T(layer)[:,:,:,channel]) # run the render_naive function and start halucinating!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZroBKE5YiDsb"},"source":["<a id=\"multiscale\"></a>\n","## Multiscale image generation\n","\n","Looks like the network wants to show us something interesting! Let's help it. We are going to apply gradient ascent on multiple scales. Details formed on smaller scale will be upscaled and augmented with additional details on the next scale.\n","\n","With multiscale image generation it may be tempting to set the number of octaves to some high value to produce wallpaper-sized images. Storing network activations and backprop values will quickly run out of GPU memory in this case. There is a simple trick to avoid this: split the image into smaller tiles and compute each tile gradient independently. Applying random shifts to the image before every iteration helps avoid tile seams and improves the overall image quality."]},{"cell_type":"code","metadata":{"id":"2iwWSOgsQglG","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378555,"user_tz":-480,"elapsed":8426,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["def tffunc(*argtypes):\n","    '''Helper that transforms TF-graph generating function into a regular one.\n","    See \"resize\" function below.\n","    '''\n","    placeholders = list(map(tf.placeholder, argtypes))\n","    def wrap(f):\n","        out = f(*placeholders)\n","        def wrapper(*args, **kw):\n","            return out.eval(dict(zip(placeholders, args)), session=kw.get('session'))\n","        return wrapper\n","    return wrap\n","\n","# Helper function that uses TF to resize an image\n","def resize(img, size):\n","    img = tf.expand_dims(img, 0)\n","    return tf.image.resize_bilinear(img, size)[0,:,:,:]\n","resize = tffunc(np.float32, np.int32)(resize)\n","\n","\n","def calc_grad_tiled(img, t_grad, tile_size=512):\n","    '''Compute the value of tensor t_grad over the image in a tiled way.\n","    Random shifts are applied to the image to blur tile boundaries over \n","    multiple iterations.'''\n","    sz = tile_size\n","    h, w = img.shape[:2] # size of the image\n","    sx, sy = np.random.randint(sz, size=2) # random shift numbers generated\n","    img_shift = np.roll(np.roll(img, sx, 1), sy, 0) #shift the whole image. np.roll = Roll array elements along a given axis\n","    grad = np.zeros_like(img)\n","    for y in range(0, max(h-sz//2, sz),sz):\n","        for x in range(0, max(w-sz//2, sz),sz):\n","            sub = img_shift[y:y+sz,x:x+sz] # get the image patch (tile)\n","            g = sess.run(t_grad, {t_input:sub}) # calculate the gradient only in the image patch not in the whole image!\n","            grad[y:y+sz,x:x+sz] = g # put the whole gradient together from the tiled gradients g\n","    return np.roll(np.roll(grad, -sx, 1), -sy, 0) # shift back"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRCJdG8gQglN","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378556,"user_tz":-480,"elapsed":8417,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["def render_multiscale(t_obj, img0=img_noise, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4):\n","    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n","    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n","    \n","    img = img0.copy()\n","    for octave in range(octave_n):\n","        if octave>0:\n","            hw = np.float32(img.shape[:2])*octave_scale # calculate new height and width (scale up by octave_scale)\n","            img = resize(img, np.int32(hw)) # rescale the image to the new size\n","        for i in range(iter_n):\n","            g = calc_grad_tiled(img, t_grad) # calculate the gradient of the new image in each pixel using the calc_grad_tiled function\n","            # normalizing the gradient, so the same step size should work \n","            g /= g.std()+1e-8         # for different layers and networks\n","            img += g*step # update the image with the gradient\n","            print('.', end = ' ')\n","            #clear_output()\n","            showarray(visstd(img))\n","\n","render_multiscale(T(layer)[:,:,:,channel])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDSZMtVYQglV"},"source":["<a id=\"laplacian\"></a>\n","## Laplacian Pyramid Gradient Normalization\n","\n","This looks better, but the resulting images mostly contain high frequencies. Can we improve it? One way is to add a smoothness prior into the optimization objective. This will effectively blur the image a little every iteration, suppressing the higher frequencies, so that the lower frequencies can catch up. This will require more iterations to produce a nice image. Why don't we just boost lower frequencies of the gradient instead? One way to achieve this is through the [Laplacian pyramid](https://en.wikipedia.org/wiki/Pyramid_%28image_processing%29#Laplacian_pyramid) decomposition. We call the resulting technique _Laplacian Pyramid Gradient Normalization_."]},{"cell_type":"code","metadata":{"id":"Do3WpFSUQglX","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378557,"user_tz":-480,"elapsed":8407,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["k = np.float32([1,4,6,4,1])\n","k = np.outer(k, k)\n","k5x5 = k[:,:,None,None]/k.sum()*np.eye(3, dtype=np.float32)\n","\n","def lap_split(img):\n","    '''Split the image into lo and hi frequency components'''\n","    with tf.name_scope('split'):\n","        lo = tf.nn.conv2d(img, k5x5, [1,2,2,1], 'SAME')\n","        lo2 = tf.nn.conv2d_transpose(lo, k5x5*4, tf.shape(img), [1,2,2,1])\n","        hi = img-lo2\n","    return lo, hi\n","\n","def lap_split_n(img, n):\n","    '''Build Laplacian pyramid with n splits'''\n","    levels = []\n","    for i in range(n):\n","        img, hi = lap_split(img)\n","        levels.append(hi)\n","    levels.append(img)\n","    return levels[::-1]\n","\n","def lap_merge(levels):\n","    '''Merge Laplacian pyramid'''\n","    img = levels[0]\n","    for hi in levels[1:]:\n","        with tf.name_scope('merge'):\n","            img = tf.nn.conv2d_transpose(img, k5x5*4, tf.shape(hi), [1,2,2,1]) + hi\n","    return img\n","\n","def normalize_std(img, eps=1e-10):\n","    '''Normalize image by making its standard deviation = 1.0'''\n","    with tf.name_scope('normalize'):\n","        std = tf.sqrt(tf.reduce_mean(tf.square(img)))\n","        return img/tf.maximum(std, eps)\n","\n","def lap_normalize(img, scale_n=4):\n","    '''Perform the Laplacian pyramid normalization.'''\n","    img = tf.expand_dims(img,0)\n","    tlevels = lap_split_n(img, scale_n)\n","    tlevels = list(map(normalize_std, tlevels))\n","    out = lap_merge(tlevels)\n","    return out[0,:,:,:]\n","\n","# Showing the lap_normalize graph with TensorBoard\n","lap_graph = tf.Graph()\n","with lap_graph.as_default():\n","    lap_in = tf.placeholder(np.float32, name='lap_in')\n","    lap_out = lap_normalize(lap_in)\n","show_graph(lap_graph)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zj8Ms-WqQgla","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378557,"user_tz":-480,"elapsed":8397,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["def render_lapnorm(t_obj, img0=img_noise, t_score2=0, visfunc=visstd,\n","                   iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):\n","    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n","    t_grad = tf.gradients(t_score+t_score2, t_input)[0] # behold the power of automatic differentiation!\n","    # build the laplacian normalization graph\n","    lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))\n","\n","    img = img0.copy()\n","    for octave in range(octave_n):\n","        if octave>0:\n","            hw = np.float32(img.shape[:2])*octave_scale\n","            img = resize(img, np.int32(hw))\n","        for i in range(iter_n):\n","            g = calc_grad_tiled(img, t_grad)\n","            g = lap_norm_func(g)\n","            img += g*step\n","            print('.', end = ' ')\n","        clear_output()\n","        showarray(visfunc(img))\n","\n","render_lapnorm(T(layer)[:,:,:,channel])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YzXJUF2lQgln"},"source":["<a id=\"playing\"></a>\n","## Playing with feature visualizations\n","\n","We got a nice smooth image using only 10 iterations per octave. In case of running on GPU this takes just a few seconds. Let's try to visualize another channel from the same layer. The network can generate wide diversity of patterns."]},{"cell_type":"code","metadata":{"id":"UkSR8Nd5pT9H","executionInfo":{"status":"aborted","timestamp":1604240378558,"user_tz":-480,"elapsed":8388,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["print(layer)\n","print(channel)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6jfiWqZQglq","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378558,"user_tz":-480,"elapsed":8378,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_lapnorm(T(layer)[:,:,:,65])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ka6RyOMEnrB5"},"source":["Lower layers produce features of lower complexity."]},{"cell_type":"code","metadata":{"id":"KYOtrJxMnlws","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378559,"user_tz":-480,"elapsed":8369,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_lapnorm(T('mixed3b_1x1_pre_relu')[:,:,:,121])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHxPi3JKpT9U","executionInfo":{"status":"aborted","timestamp":1604240378560,"user_tz":-480,"elapsed":8359,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_lapnorm(-T('mixed3b_1x1_pre_relu')[:,:,:,121])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wuP8a4FlQglx"},"source":["There are many interesting things one may try. For example, optimizing a linear combination of features often gives a \"mixture\" pattern."]},{"cell_type":"code","metadata":{"id":"ozN-nH2yQgl0","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378560,"user_tz":-480,"elapsed":8348,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_lapnorm(T(layer)[:,:,:,65]+T(layer)[:,:,:,139], octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUtE8ZsfpT9b","executionInfo":{"status":"aborted","timestamp":1604240378561,"user_tz":-480,"elapsed":8339,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["img0 = PIL.Image.open('116.jpg')\n","img0 = np.float32(img0)\n","\n","render_lapnorm(T(layer)[:,:,:,65]/T(layer)[:,:,:,139]-T(layer)[:,:,:,65]*T(layer)[:,:,:,139], octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xB2oK86VpT9d","executionInfo":{"status":"aborted","timestamp":1604240378561,"user_tz":-480,"elapsed":8328,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_lapnorm(-T(layer)[:,:,:,65]**2 + T(layer)[:,:,:,139]**2, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pSGF2LeypT9h","executionInfo":{"status":"aborted","timestamp":1604240378562,"user_tz":-480,"elapsed":8319,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_lapnorm(T('mixed3b_3x3_pre_relu')[:,:,:,41], octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q32ASbGCpT9i","executionInfo":{"status":"aborted","timestamp":1604240378563,"user_tz":-480,"elapsed":8309,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_lapnorm(T('mixed4e_5x5_pre_relu')[:,:,:,50], octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jYw5oXNIpT9m"},"source":["We can even combine features from different layers by obtaining just the score of one and passing it to the other to jointly calculate the gradient"]},{"cell_type":"code","metadata":{"id":"WNaMLlhbpT9m","executionInfo":{"status":"aborted","timestamp":1604240378563,"user_tz":-480,"elapsed":8305,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["def mean_score(t_obj):\n","    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n","    return t_score\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHa85bDQpT9o","executionInfo":{"status":"aborted","timestamp":1604240378564,"user_tz":-480,"elapsed":8296,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["feat_layer_comb = mean_score(T('mixed3b_3x3_pre_relu')[:,:,:,41])\n","\n","render_lapnorm(T('mixed4e_5x5_pre_relu')[:,:,:,50], t_score2=feat_layer_comb, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94hkOJE-pT9q","executionInfo":{"status":"aborted","timestamp":1604240378564,"user_tz":-480,"elapsed":8286,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["feat_layer_comb = mean_score(T('mixed3b_3x3_pre_relu')[:,:,:,41])\n","\n","render_lapnorm(T('mixed4e_5x5_pre_relu')[:,:,:,50], t_score2=-feat_layer_comb, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7iqr726pT9s","executionInfo":{"status":"aborted","timestamp":1604240378565,"user_tz":-480,"elapsed":8276,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["feat_layer_comb = mean_score(T('mixed3b_3x3_pre_relu')[:,:,:,41])\n","\n","render_lapnorm(-T('mixed4e_5x5_pre_relu')[:,:,:,50], t_score2=feat_layer_comb, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRhhMZYjpT9w","executionInfo":{"status":"aborted","timestamp":1604240378566,"user_tz":-480,"elapsed":8267,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["feat_layer_comb = mean_score(T('mixed3b_3x3_pre_relu')[:,:,:,41])\n","\n","render_lapnorm(-T('mixed4e_5x5_pre_relu')[:,:,:,50], t_score2=-feat_layer_comb, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlpIDR24pT9y"},"source":["Experiment with different optimizations (reduce_max/sum/etc) and weighting the scores"]},{"cell_type":"code","metadata":{"id":"cYubGksvpT90","executionInfo":{"status":"aborted","timestamp":1604240378566,"user_tz":-480,"elapsed":8261,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["# TO-DO: incorporate the mean score func internall below. Add flag for \"sum_score\" function as well (incorporate too)\n","\n","def render_lap2_mean_grad(t_obj, img0=img_noise, t_score2=0, op='add', wght1=0.5, wght2=0.5, visfunc=visstd,\n","                   iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):\n","    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n","    \n","    if op=='add':\n","        t_grad = tf.gradients(wght1*t_score+wght2*t_score2, t_input)[0] # behold the power of automatic differentiation!\n","    if op=='subtract':\n","        t_grad = tf.gradients(wght1*t_score-wght2*t_score2, t_input)[0] # behold the power of automatic differentiation!\n","    # build the laplacian normalization graph\n","    lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))\n","\n","    img = img0.copy()\n","    for octave in range(octave_n):\n","        if octave>0:\n","            hw = np.float32(img.shape[:2])*octave_scale\n","            img = resize(img, np.int32(hw))\n","        for i in range(iter_n):\n","            g = calc_grad_tiled(img, t_grad)\n","            g = lap_norm_func(g)\n","            img += g*step\n","            print('.', end = ' ')\n","        clear_output()\n","        showarray(visfunc(img))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CxFM394pT92","executionInfo":{"status":"aborted","timestamp":1604240378567,"user_tz":-480,"elapsed":8252,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["feat_layer_comb = mean_score(T('mixed3b_3x3_pre_relu')[:,:,:,41])\n","\n","render_lap2_mean_grad(T('mixed4e_5x5_pre_relu')[:,:,:,50], t_score2=feat_layer_comb, wght1=0.2, wght2=0.8, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWQek4kUpT97"},"source":["Try adding different noise images"]},{"cell_type":"code","metadata":{"id":"UdXnAv8NpT97","executionInfo":{"status":"aborted","timestamp":1604240378567,"user_tz":-480,"elapsed":8241,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_lap2_mean_grad(T('mixed4e_5x5_pre_relu')[:,:,:,50], t_score2=feat_layer_comb, wght1=0.4, wght2=0.6, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2i-6iylpT-A","executionInfo":{"status":"aborted","timestamp":1604240378568,"user_tz":-480,"elapsed":8231,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["feat_layer_comb = mean_score(T('mixed3b_3x3_pre_relu')[:,:,:,41])\n","render_lap2_mean_grad(T('mixed4e_5x5_pre_relu')[:,:,:,50], t_score2=feat_layer_comb, op='subtract', wght1=0.4, wght2=0.6, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0Zgi7QnpT-C","executionInfo":{"status":"aborted","timestamp":1604240378568,"user_tz":-480,"elapsed":8221,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["feat_layer_comb = mean_score(T('mixed4e_5x5_pre_relu')[:,:,:,50])\n","render_lap2_mean_grad(T('mixed3b_3x3_pre_relu')[:,:,:,41], t_score2=feat_layer_comb, op='subtract', wght1=0.4, wght2=0.6, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPcMc_ytpT-G","executionInfo":{"status":"aborted","timestamp":1604240378569,"user_tz":-480,"elapsed":8212,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["norm_noise = np.absolute(np.random.normal(loc =0.0, scale =0.25, size=(224,224,3)))\n","render_lap2_mean_grad(T('mixed4e_5x5_pre_relu')[:,:,:,50], img0=-norm_noise, t_score2=feat_layer_comb, op='add', wght1=0.5, wght2=0.5, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9Pw31-fpT-J"},"source":["Or an actual image"]},{"cell_type":"code","metadata":{"id":"OpIyIFhCpT-K","executionInfo":{"status":"aborted","timestamp":1604240378570,"user_tz":-480,"elapsed":8210,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["def preprocess(img, crop=True, changesize=True, dsize=(224, 224)):\n","    \"\"\"Summary\n","\n","    Parameters\n","    ----------\n","    img : TYPE\n","        Description\n","    crop : bool, optional\n","        Description\n","    resize : bool, optional\n","        Description\n","    dsize : tuple, optional\n","        Description\n","\n","    Returns\n","    -------\n","    TYPE\n","        Description\n","    \"\"\"\n","    if img.dtype == np.uint8:\n","        img = img / 255.0\n","\n","    if crop:\n","        short_edge = min(img.shape[:2])\n","        yy = int((img.shape[0] - short_edge) / 2)\n","        xx = int((img.shape[1] - short_edge) / 2)\n","        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n","    else:\n","        crop_img = img\n","\n","    if changesize:\n","        norm_img = resize(crop_img, dsize, preserve_range=True)\n","    else:\n","        norm_img = crop_img\n","\n","    return (norm_img).astype(np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVeICGQWpT-M","executionInfo":{"status":"aborted","timestamp":1604240378570,"user_tz":-480,"elapsed":8200,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["# Interesting that polkadot images seem to \"grow\" humans ('mixed4e_5x5_pre_relu')\n","# TO-DO make our own 'polkadot' images instead of lossy compressed images\n","# TO-DO \"dupe\" the network into drawing humans, but do not include the polkadot image in the representation\n","img0 = np.asarray(PIL.Image.open('polkadots2.jpg'))[..., :3].astype(np.float32)\n","img0 = preprocess(img0)\n","render_lap2_mean_grad(T('mixed4e_5x5_pre_relu')[:,:,:,50],img0=img0, t_score2=feat_layer_comb, op='add', wght1=0.8, wght2=0.2, octave_n=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5hJlRtuBpT-Q"},"source":["Now see if you can create a track from lower layers to higher layers and detect how features transform throughout. When are things recognizable? What do you think the textures' purposes are? Etc.\n","\n","We can also begin to create a taxonomy of sorts by describing the features found in each layer and channel we explore. Let's start an open document to add to. Explore yourself and/or by these labeled (but undescribed) images from each [layer/channel](http://storage.googleapis.com/deepdream/visualz/tensorflow_inception/index.html).\n","\n","Another idea is to create a gif that contains the shifting of weights from 2 feature spaces, say from 0. and 1. of feature space A and B to 1. and 0. "]},{"cell_type":"markdown","metadata":{"id":"lcPe-ZMv0dYR"},"source":["<a id=\"deepdream\"></a>\n","## DeepDream\n","\n","Now let's reproduce the [DeepDream algorithm](https://github.com/google/deepdream/blob/master/dream.ipynb) with TensorFlow. \n"]},{"cell_type":"code","metadata":{"id":"qM2U_96hyUwN","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378571,"user_tz":-480,"elapsed":8198,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["def render_deepdream(t_obj, img0=img_noise,\n","                     iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):\n","    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n","    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n","\n","    # split the image into a number of octaves getting smaller and smaller images\n","    img = img0\n","    octaves = []\n","    for i in range(octave_n-1):\n","        hw = img.shape[:2] #image height and width\n","        lo = resize(img, np.int32(np.float32(hw)/octave_scale)) #low frequency parts (smaller image)\n","        hi = img-resize(lo, hw) #high frequency parts (details)\n","        img = lo # next iteration rescale this one\n","        octaves.append(hi) # add the details to octaves\n","    \n","    # generate details octave by octave from samll image to large\n","    for octave in range(octave_n):\n","        if octave>0:\n","            hi = octaves[-octave]\n","            img = resize(img, hi.shape[:2])+hi\n","        for i in range(iter_n):\n","            g = calc_grad_tiled(img, t_grad)\n","            img += g*(step / (np.abs(g).mean()+1e-7))\n","            print('.',end = ' ')\n","            #clear_output()\n","            showarray(img/255.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZH3UcLnpT-V"},"source":["Let's load some image and populate it with DogSlugs (in case you've missed them)."]},{"cell_type":"code","metadata":{"id":"M9_vOh_2Qgl-","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378571,"user_tz":-480,"elapsed":8187,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["img0 = PIL.Image.open('118.jpg')\n","img0 = np.float32(img0)\n","showarray(img0/255)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k0oggbGEeC3U","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378572,"user_tz":-480,"elapsed":8177,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_deepdream(tf.square(T('mixed4c')), img0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJzvhEFxpB7E"},"source":["Note that results can differ from the [Caffe](https://github.com/BVLC/caffe)'s implementation, as we are using an independently trained network. Still, the network seems to like dogs and animal-like features due to the nature of the ImageNet dataset.\n","\n","Using an arbitrary optimization objective still works:"]},{"cell_type":"code","metadata":{"id":"4GexZuwJdDmu","cellView":"both","executionInfo":{"status":"aborted","timestamp":1604240378572,"user_tz":-480,"elapsed":8167,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":["render_deepdream(T(layer)[:,:,:,139], img0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeGzptQXpT-e","executionInfo":{"status":"aborted","timestamp":1604240378573,"user_tz":-480,"elapsed":8165,"user":{"displayName":"Jonathan Sherman","photoUrl":"","userId":"12304095466054513505"}}},"source":[""],"execution_count":null,"outputs":[]}]}